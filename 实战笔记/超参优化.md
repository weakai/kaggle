# 超参优化

## 网格超参搜索

```python
from sklearn.model_selection import GridSearchCV
from sklearn.neighbors import KNeighborsClassifier

parameters_KNN = {'n_neighbors': (10, 15, 17), }
grid_KNN = GridSearchCV(KNeighborsClassifier(), parameters_KNN, cv=5,
                        n_jobs=-1, verbose=1)
grid_KNN.fit(data_tfidf_train, label_train)

print(grid_KNN.best_params_)
print(grid_KNN.best_score_)
```

精度作为网格搜索的指标

- accuracy@dafault
- precision
- recall

```python
scoring = 'precision'
grid_KNN = GridSearchCV(pipe_KNN, parameters_KNN, cv=5,
                          scoring=scoring, n_jobs=-1, verbose=1)
```

## hyperopt 超参数优化

```python
from hyperopt import fmin, hp, tpe, Trials, space_eval, STATUS_OK, STATUS_RUNNING

# Define searched space
hyper_space = {'objective': 'regression',
               'metric':'mae',
               'boosting':'gbdt',
               #'n_estimators': hp.choice('n_estimators', [25, 40, 50, 75, 100, 250, 500]),
               'max_depth':  hp.choice('max_depth', [5, 8, 10, 12, 15]),
               'num_leaves': hp.choice('num_leaves', [100, 250, 500, 650, 750, 1000,1300]),
               'subsample': hp.choice('subsample', [.3, .5, .7, .8, 1]),
               'colsample_bytree': hp.choice('colsample_bytree', [ .6, .7, .8, .9, 1]),
               'learning_rate': hp.choice('learning_rate', [.1, .2, .3]),
               'reg_alpha': hp.choice('reg_alpha', [.1, .2, .3, .4, .5, .6]),
               'reg_lambda':  hp.choice('reg_lambda', [.1, .2, .3, .4, .5, .6]),               
               'min_child_samples': hp.choice('min_child_samples', [20, 45, 70, 100])}

lgtrain = lightgbm.Dataset(X_train, label=y_train)
lgval = lightgbm.Dataset(X_val, label=y_val)

# Defining the Metric to score our optimizer
def metric(df, preds):
    df['diff'] = (df['scalar_coupling_constant'] - preds).abs()
    return np.log(df.groupby('type')['diff'].mean().map(lambda x: max(x, 1e-9))).mean()

def evaluate_metric(params):
    model_lgb = lightgbm.train(params, lgtrain, 500, 
                          valid_sets=[lgtrain, lgval], early_stopping_rounds=20, 
                          verbose_eval=500)
    pred = model_lgb.predict(X_val)
    score = metric(df_val, pred)
    print(score)
    return {
        'loss': score,
        'status': STATUS_OK,
        'stats_running': STATUS_RUNNING
    }

# Initiating the optimizer

# Trail
trials = Trials()
# Set algoritm parameters
algo = partial(tpe.suggest, 
               n_startup_jobs=-1)
# Seting the number of Evals
MAX_EVALS= 15
# Fit Tree Parzen Estimator
best_vals = fmin(evaluate_metric, space=hyper_space, verbose=1,
                 algo=algo, max_evals=MAX_EVALS, trials=trials)
# Print best parameters
best_params = space_eval(hyper_space, best_vals)
# The hyper-parameters that we got in Hyperopt
print("BEST PARAMETERS: " + str(best_params))
```

预测

```python
model_lgb = lightgbm.train(best_params, lgtrain, 4000, 
                      valid_sets=[lgtrain, lgval], early_stopping_rounds=30, 
                      verbose_eval=500)

lgb_pred = model_lgb.predict(X_test)

df_test['scalar_coupling_constant'] = lgb_pred

df_test[['id', 'scalar_coupling_constant']].to_csv("molecular_struct_sub.csv", index=False)
```
