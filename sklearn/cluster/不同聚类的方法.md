---
title: clustering
---

## K-means

separate samples in n groups of equal variance, minimizing a criterion known as the inertia or within-cluster sum-of-squares

The means are commonly called the cluster “centroids”

The K-means algorithm aims to choose centroids that minimise the inertia, or within-cluster sum-of-squares criterion

Inertia is not a normalized metric: we just know that lower values are better and zero is optimal. But in very high-dimensional spaces, Euclidean distances tend to become inflated (this is an instance of the so-called “curse of dimensionality”). Running a dimensionality reduction algorithm such as Principal component analysis (PCA) prior to k-means clustering can alleviate this problem and speed up the computations.

K-means is often referred to as Lloyd’s algorithm. 

In basic terms, the algorithm has three steps.
The first step chooses the initial centroids, with the most basic method being to choose  samples from the dataset.
After initialization, K-means consists of looping between the two other steps. The first step assigns each sample to its nearest centroid. The second step creates new centroids by taking the mean value of all of the samples assigned to each previous centroid. The difference between the old and the new centroids are computed and the algorithm repeats these last two steps until this value is less than a threshold. In other words, it repeats until the centroids do not move significantly.

K-means is equivalent to the expectation-maximization algorithm with a small, all-equal, diagonal covariance matrix.

The algorithm can also be understood through the concept of Voronoi diagrams.

Given enough time, K-means will always converge, however this may be to a local minimum. This is highly dependent on the initialization of the centroids. As a result, the computation is often done several times, with different initializations of the centroids. One method to help address this issue is the k-means++ initialization scheme, which has been implemented in scikit-learn (use the init='k-means++' parameter). This initializes the centroids to be (generally) distant from each other, leading to probably better results than random initialization, as shown in the reference.

K-means++ can also be called independently to select seeds for other clustering algorithms

## Mini Batch K-Means

The MiniBatchKMeans is a variant of the KMeans algorithm which uses mini-batches to reduce the computation time, while still attempting to optimise the same objective function. 

## Affinity Propagation

AffinityPropagation 通过在成对的样本之间发送消息来创建集群，直到聚合。然后使用少量的范例来描述数据集，这些范例被识别为其他样本中最具代表性的那些。在两个对之间发送的消息表示一个样本是否适合作为另一个样本的范例，这将根据来自其他对的值进行更新。这种更新迭代进行，直到收敛，此时最终的样本被选择，因此最终的聚类被给出。

Affinity Propagation 可能很有趣，因为它根据提供的数据选择集群的数量。为此目的，两个重要的参数是偏好，它控制使用多少个范例，以及 damping factor which damps the responsibility and availability messages to avoid numerical oscillations when updating these messages.

主要缺点:复杂性,时间复杂度为阶数。此外，当使用密集相似矩阵时，记忆复杂度是有序的，但当使用稀疏相似矩阵时，记忆复杂度是可约化的。这使得亲和传播最适合中小规模的数据集。

## Mean Shift

MeanShift 聚类的目的是在平滑密度的样本中发现斑点。它是一种基于质心的算法，其工作原理是更新候选质心为给定区域内点的均值。这些候选元素在后期处理阶段被过滤，以消除接近重复的元素，形成最终的质心集。

## Spectral clustering

